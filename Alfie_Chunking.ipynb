{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Sprint 1: High-Performance Data Ingestion & Retrieval\n",
        "Data: 19,187 Semantic Chunks from Algebra Textbooks.\n",
        "\n",
        "Tech: HuggingFace Embeddings (all-mpnet-base-v2), BM25 Keyword Search, and CrossEncoder Reranking.\n",
        "\n",
        "Status: Production-ready vector foundation complete."
      ],
      "metadata": {
        "id": "B0CHI4jJlqml"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9zksnzBbxg2"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install LangChain, PDF loaders, and the Semantic Chunker\n",
        "!pip install -qU langchain_experimental langchain_openai langchain_community pypdf tiktoken"
      ],
      "metadata": {
        "id": "Xc540Wvyb4_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Semantic Ingestion Script\n",
        "\n",
        "Summary of the Flow:\n",
        "Find a PDF.\n",
        "\n",
        "Extract the text and page numbers.\n",
        "Analyze the meaning (using your all-mpnet-base-v2 model).\n",
        "\n",
        "Cut the text into chunks based on topic shifts.\n",
        "Store those chunks in a list with their \"ID card\" (filename and page number)."
      ],
      "metadata": {
        "id": "kD6TUez7cFmv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install HuggingFace and Sentence Transformers\n",
        "import os\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "\n",
        "# 1. Initialize Local Embeddings (No API Key Required)\n",
        "# We use 'all-mpnet-base-v2' because it's the gold standard for academic sentence similarity\n",
        "print(\"Initializing local embedding model...\")\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
        "\n",
        "# 2. Setup the Semantic Chunker\n",
        "# 'percentile' threshold helps capture topic shifts in structured textbooks\n",
        "text_splitter = SemanticChunker(embeddings, breakpoint_threshold_type=\"percentile\")\n",
        "\n",
        "# 3. Path Configuration\n",
        "DATA_FOLDER = \"/content/drive/MyDrive/Alfie/Data\"\n",
        "all_chunks = []\n",
        "\n",
        "# 4. The Ingestion Loop\n",
        "print(\"Starting PDF Ingestion...\")\n",
        "for filename in os.listdir(DATA_FOLDER): #os.listdir() method is used to retrieve a list containing the names of all entries (files and directories) within a specified path.\n",
        "    if filename.endswith(\".pdf\"):\n",
        "        file_path = os.path.join(DATA_FOLDER, filename)\n",
        "        print(f\"--- Processing: {filename} ---\")\n",
        "\n",
        "        try:\n",
        "            loader = PyPDFLoader(file_path) # This \"opens\" the PDF and extracts the raw text. It also captures metadata (like which page number the text came from).\n",
        "            pages = loader.load()\n",
        "\n",
        "            # We split the entire document while keeping track of page numbers\n",
        "            chunks = text_splitter.split_documents(pages)\n",
        "\n",
        "            # Append to our master list\n",
        "            all_chunks.extend(chunks)\n",
        "            print(f\"Added {len(chunks)} chunks from {filename}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {filename}: {e}\")\n",
        "\n",
        "print(f\"\\n✅ Success! Total semantic chunks created: {len(all_chunks)}\")\n",
        "\n",
        "# Quick Check: Look at the first chunk's metadata\n",
        "if all_chunks:\n",
        "    print(f\"Sample Metadata: {all_chunks[0].metadata}\")"
      ],
      "metadata": {
        "id": "35MtU9eEdBF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8ehb5woNtxuu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "STORAGE PHASE - Saving the chunks to vectorDB\n"
      ],
      "metadata": {
        "id": "46XYIJI5fsMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Install ChromaDB\n",
        "!pip install -qU chromadb\n",
        "#ChromaDB: This is an open-source Vector Database.\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "# 2. Define where to save the database on your Drive\n",
        "PERSIST_DIRECTORY = \"/content/drive/MyDrive/Alfie/VectorDB\"\n",
        "\n",
        "# 3. Create the Vector Store and Save the Chunks\n",
        "print(\"Creating Vector Database... this might take a few minutes for 6000+ chunks.\")\n",
        "vector_db = Chroma.from_documents(\n",
        "    documents=all_chunks,\n",
        "    embedding=embeddings, # The HuggingFace model we initialized earlier\n",
        "    persist_directory=PERSIST_DIRECTORY\n",
        ")\n",
        "\n",
        "# 4. Finalize the save\n",
        "vector_db.persist() # Fuck it we can remove this since we are already saing it in the drive.\n",
        "print(f\"✅ Database saved to {PERSIST_DIRECTORY}\")"
      ],
      "metadata": {
        "id": "VE_YEVkMfrTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entire flow so far -\n",
        "\n",
        "Sentence Splitting: The PDF text is broken into individual sentences.\n",
        "\n",
        "Temporary Embedding: Each sentence is turned into a vector just to calculate where to cut.\n",
        "\n",
        "Semantic Chunking: Using Cosine Similarity, sentences are grouped into chunks (e.g., sentences 1-5 become \"Chunk A\").\n",
        "\n",
        "Final Embedding: Once \"Chunk A\" is created, the entire chunk is sent to the embedding model one last time to get a single \"Master Vector\" representing that whole paragraph.\n",
        "\n",
        "Storage: That Master Vector + the Text + the Metadata (page number/filename) are saved into ChromaDB.\n",
        "\n",
        "Why do we embed a second time?\n",
        "We do this because a single vector representing a whole paragraph is much more powerful for search than just having a bunch of individual sentence vectors. It captures the \"full context\" of the idea.\n"
      ],
      "metadata": {
        "id": "c3M4zrKNzPw9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load the DB (This proves it saved correctly)\n",
        "vector_db = Chroma(\n",
        "    persist_directory=\"/content/drive/MyDrive/Alfie/VectorDB\",\n",
        "    embedding_function=embeddings\n",
        ")\n",
        "\n",
        "# 2. Check the count\n",
        "print(f\"Verified Chunks in DB: {vector_db._collection.count()}\")\n",
        "\n",
        "# 3. Test Retrieval Quality\n",
        "query = \"What are the rules for adding and subtracting polynomials?\"\n",
        "results = vector_db.similarity_search(query, k=3)\n",
        "\n",
        "print(\"\\n--- Top Result ---\")\n",
        "if results:\n",
        "    print(f\"Source: {results[0].metadata.get('source')}\")\n",
        "    print(f\"Content Snippet: {results[0].page_content[:400]}...\")\n",
        "else:\n",
        "    print(\"❌ No results found. Something went wrong with the index!\")"
      ],
      "metadata": {
        "id": "YaU5w3wbclCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b895b2de"
      },
      "source": [
        "# Task\n",
        "Create a hybrid retrieval system by manually combining and deduplicating results from both BM25 and vector retrievers, then apply a reranking model to these combined documents and display the reranked results, explaining the benefits of this approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a50cc387"
      },
      "source": [
        "## Manual Combination and Deduplication\n",
        "\n",
        "### Subtask:\n",
        "Execute both the BM25 retriever and the vector retriever separately, then combine their results and remove any duplicate documents. This will provide a comprehensive list of potentially relevant documents.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e36772f3"
      },
      "source": [
        "**Reasoning**:\n",
        "Execute both BM25 and vector retrievers separately for the given query, then combine their results and deduplicate them to fulfill the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fa7f3cd"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous cell failed because `bm25_retriever` and `vector_retriever` were not defined. This happened because the cell where they were initialized (`ZbIMELbkiwQ8`) failed due to an `ImportError` related to `EnsembleRetriever`. To fix this, I will re-initialize `bm25_retriever` and `vector_retriever` in the current cell, omitting the problematic `EnsembleRetriever` part as it's not needed for the current manual deduplication subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87fae8a5"
      },
      "source": [
        "from langchain_community.retrievers import BM25Retriever\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "query = \"What is the FOIL method?\"\n",
        "\n",
        "# Re-initialize the retrievers as their previous initialization failed\n",
        "# 1. Initialize the Keyword Retriever (BM25)\n",
        "bm25_retriever = BM25Retriever.from_documents(all_chunks)\n",
        "bm25_retriever.k = 3 # Top 3 keyword matches\n",
        "\n",
        "# 2. Re-load Chroma and Initialize the Vector Retriever\n",
        "# `embeddings` and `PERSIST_DIRECTORY` are available from previous cells.\n",
        "vector_db = Chroma(\n",
        "    persist_directory=PERSIST_DIRECTORY,\n",
        "    embedding_function=embeddings\n",
        ")\n",
        "vector_retriever = vector_db.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "\n",
        "# 1. Use the existing bm25_retriever to retrieve documents\n",
        "bm25_docs = bm25_retriever.invoke(query)\n",
        "print(f\"BM25 retrieved {len(bm25_docs)} documents.\")\n",
        "\n",
        "# 2. Use the existing vector_retriever to retrieve documents\n",
        "vector_docs = vector_retriever.invoke(query)\n",
        "print(f\"Vector retrieved {len(vector_docs)} documents.\")\n",
        "\n",
        "# 3. Combine bm25_docs and vector_docs into a single list\n",
        "combined_docs = bm25_docs + vector_docs\n",
        "print(f\"Combined documents before deduplication: {len(combined_docs)}\")\n",
        "\n",
        "# 4. Create a new list to store deduplicated documents\n",
        "deduplicated_docs = []\n",
        "seen_page_content = set()\n",
        "\n",
        "for doc in combined_docs:\n",
        "    if doc.page_content not in seen_page_content:\n",
        "        deduplicated_docs.append(doc)\n",
        "        seen_page_content.add(doc.page_content)\n",
        "\n",
        "# 5. Print the number of documents before and after deduplication\n",
        "print(f\"Combined documents after deduplication: {len(deduplicated_docs)}\")\n",
        "\n",
        "print(\"\\n--- Deduplicated Results ---\")\n",
        "for i, doc in enumerate(deduplicated_docs):\n",
        "    print(f\"Result {i+1} (Source: {doc.metadata.get('source', 'N/A')} - Page: {doc.metadata.get('page_label', 'N/A')}):\")\n",
        "    print(f\"{doc.page_content[:200]}...\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbedff5e"
      },
      "source": [
        "## Implement Reranking\n",
        "\n",
        "### Subtask:\n",
        "Install a suitable reranking library (e.g., using a cross-encoder from `sentence-transformers`), load a reranker model, and then apply it to the combined and deduplicated documents to reorder them based on their relevance to the query.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "117c09dc"
      },
      "source": [
        "**Reasoning**:\n",
        "First, I need to install the `sentence-transformers` library to use the `CrossEncoder` for reranking.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27f7de91"
      },
      "source": [
        "pip install -qU sentence-transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "636fcdf1"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that `sentence-transformers` is installed, I will import `CrossEncoder`, load the specified reranker model, prepare the input for reranking by pairing the query with each deduplicated document's content, apply the reranker to get relevance scores, and then sort the documents based on these scores to obtain the reranked results. I will then print the reranked results along with their sources and content snippets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bf78047"
      },
      "source": [
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "# 1. Import and Load a pre-trained cross-encoder model\n",
        "# Using 'cross-encoder/ms-marco-MiniLM-L-6-v2' as suggested\n",
        "print(\"Loading CrossEncoder model...\")\n",
        "reranker_model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
        "print(\"CrossEncoder model loaded.\")\n",
        "\n",
        "# 2. Prepare input for the reranker\n",
        "# The reranker expects a list of (query, document_text) pairs\n",
        "reranker_inputs = [[query, doc.page_content] for doc in deduplicated_docs]\n",
        "\n",
        "# 3. Get relevance scores from the reranker\n",
        "print(\"Reranking documents...\")\n",
        "relevance_scores = reranker_model.predict(reranker_inputs)\n",
        "print(\"Documents reranked.\")\n",
        "\n",
        "# 4. Pair documents with their scores and sort them\n",
        "doc_score_pairs = list(zip(deduplicated_docs, relevance_scores))\n",
        "\n",
        "# Sort in descending order based on relevance score\n",
        "reranked_docs_with_scores = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Store the reranked documents (without scores for a clean list)\n",
        "reranked_docs = [doc for doc, score in reranked_docs_with_scores]\n",
        "\n",
        "print(\"\\n--- Reranked Results ---\")\n",
        "for i, (doc, score) in enumerate(reranked_docs_with_scores):\n",
        "    print(f\"Result {i+1} (Score: {score:.4f}, Source: {doc.metadata.get('source', 'N/A')} - Page: {doc.metadata.get('page_label', 'N/A')}):\")\n",
        "    print(f\"{doc.page_content[:200]}...\\n\")\n",
        "\n",
        "print(\"\\nBenefits of this approach:\")\n",
        "print(\"- **Improved Relevance**: Reranking models are trained to identify fine-grained semantic matches, often improving the quality of top results compared to initial retrieval.\")\n",
        "print(\"- **Hybrid Strength**: Combining keyword (BM25) and semantic (vector) retrieval ensures broad coverage, while reranking refines the results for precision.\")\n",
        "print(\"- **Better User Experience**: More relevant results at the top lead to a more efficient and satisfying information retrieval experience.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da442fc9"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Display the reranked documents and explain the benefits of this hybrid retrieval approach.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d613a3d"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "The task asked to display the reranked documents and explain the benefits of the hybrid retrieval approach. This was successfully achieved by showing the reordered list of documents with their relevance scores and explicitly listing the advantages.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   Initially, both BM25 and vector retrievers were re-initialized due to a `NameError`.\n",
        "*   For the query \"What is the FOIL method?\", the BM25 retriever retrieved 3 documents, and the vector retriever also retrieved 3 documents.\n",
        "*   After combining the results from both retrievers, the total count was 6 documents.\n",
        "*   Deduplication based on `page_content` reduced the combined list to 5 unique documents, indicating at least one overlap between the two retrieval methods.\n",
        "*   A `CrossEncoder` model, specifically 'cross-encoder/ms-marco-MiniLM-L-6-v2', was successfully loaded and used for reranking.\n",
        "*   The reranking process effectively reordered the 5 deduplicated documents based on their relevance to the query.\n",
        "*   Documents highly relevant to the \"FOIL method\" received high relevance scores (e.g., 7.7543, 6.3138), placing them at the top.\n",
        "*   Less relevant documents received significantly lower scores (e.g., 0.4131, -10.6818), indicating their lower priority.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The hybrid retrieval system, combining BM25, vector search, and a cross-encoder reranker, provides a robust method for retrieving highly relevant documents by leveraging both keyword and semantic matching, followed by fine-grained relevance scoring.\n",
        "*   Further evaluation could involve quantitative metrics (e.g., NDCG, MRR) against a gold standard to objectively measure the improvement in retrieval effectiveness provided by the reranking step.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Full Pipeline So Far:\n",
        "Step\tAction\tResult\n",
        "1. Ingestion\tSemantic Chunking\t6,000 meaningful text snippets.\n",
        "2. Storage\tChromaDB\tVector coordinates for all snippets.\n",
        "3. Retrieval\tBM25 + Vector\tA \"rough list\" of the 6 most likely chunks.\n",
        "4. Reranking\tCross-Encoder\tThe 1 absolute best chunk moved to the top."
      ],
      "metadata": {
        "id": "a3TISHUR2cUI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pEDAXsCj2YjT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}